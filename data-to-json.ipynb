{
 "metadata": {
  "name": "",
  "signature": "sha256:f93d242916ea1cbf3505d08c2d661cdeae1b0f6a03a2ef4384defb3e066d0761"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime, dateutil.parser, json, math, urllib2\n",
      "from dateutil import tz\n",
      "from TileIndex import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fetch data for ACHD sites\n",
      "========================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def epoch_time(dt):\n",
      "    return (dt - datetime.datetime(1970, 1, 1, tzinfo=tz.tzutc())).total_seconds()\n",
      "\n",
      "def fetch_multi_tile(multifeed_name, tileindex):\n",
      "    url = 'http://esdr.cmucreatelab.org/api/v1/multifeeds/%s/tiles/%d.%d' % (multifeed_name, tileindex.level, tileindex.offset)\n",
      "    return json.loads(urllib2.urlopen(url).read())\n",
      "\n",
      "def get_achd_samples(multifeed_name, start_time, end_time):\n",
      "    sample_width = 3600\n",
      "    start_tile = TileIndex.tile_for_time_and_sample_width(start_time, sample_width)\n",
      "    end_tile = TileIndex.tile_for_time_and_sample_width(end_time, sample_width)\n",
      "    assert start_tile == end_tile # TODO: cope with this case -- need new code to iterate\n",
      "\n",
      "    tile = fetch_multi_tile(multifeed_name, start_tile)\n",
      "    new_rows = []\n",
      "    for row in tile['data']:\n",
      "        if any(x != None for x in row[1:]):\n",
      "            new_time = int((row[0] + 1024 - 1800.0) / 3600) * 3600 + 1800\n",
      "            if start_time <= new_time and new_time < end_time:\n",
      "                new_rows.append([new_time] + row[1:])\n",
      "    tile['data'] = new_rows\n",
      "    return tile\n",
      "\n",
      "def accumulate_achd_samples(multifeed_name, start_time, end_time):\n",
      "    data = get_achd_samples(multifeed_name, start_time, end_time)\n",
      "    for i, full_channel_name in enumerate(data['full_channel_names']):\n",
      "        (uid, feed, name) = full_channel_name.split('.')\n",
      "        feed_id = int(feed.split('_')[1])\n",
      "        channel_vals = [row[i + 1] for row in data['data']]\n",
      "        if any(x != None for x in channel_vals):\n",
      "            site_info[feed_id][name] = channel_vals\n",
      "\n",
      "def find_site(name):\n",
      "    for key, val in site_info.iteritems():\n",
      "        if val['name'] == name:\n",
      "            return val\n",
      "    raise Exception('Cannot find site %s' % name)\n",
      "\n",
      "def merge_sites(dest_name, add_name):\n",
      "    print 'Merging %s into %s' % (add_name, dest_name)\n",
      "    dest = find_site(dest_name)\n",
      "    add = find_site(add_name)\n",
      "    for key, val in add.iteritems():\n",
      "        if key in dest:\n",
      "            print '  Skipping key %s already in dest' % key\n",
      "        else:\n",
      "            dest[key] = val\n",
      "    del site_info[add['id']]\n",
      "\n",
      "def write_json_for_date(date):\n",
      "    global site_info\n",
      "    # Get id, lat, lon, name for each feed\n",
      "    url = 'http://esdr.cmucreatelab.org/api/v1/feeds?where=productId=1,minTimeSecs>0&fields=id,name,latitude,longitude'\n",
      "    rows = json.loads(urllib2.urlopen(url).read())['data']['rows']\n",
      "    site_info = {row['id']: row for row in rows}\n",
      "\n",
      "    # Get data for each feed\n",
      "    start_datetime = dateutil.parser.parse(date).replace(tzinfo=tz.gettz('America/New_York'))\n",
      "    start_time = epoch_time(start_datetime)\n",
      "    end_time = start_time + 86400\n",
      "\n",
      "    accumulate_achd_samples('achd_wind_speed', start_time, end_time)\n",
      "    accumulate_achd_samples('achd_wind_direction', start_time, end_time)\n",
      "    accumulate_achd_samples('achd_pm_25', start_time, end_time)\n",
      "    accumulate_achd_samples('achd_pm_10', start_time, end_time)\n",
      "\n",
      "    merge_sites('Lawrenceville ACHD', 'Lawrenceville 2 ACHD')\n",
      "    merge_sites('Liberty ACHD', 'Liberty 2 ACHD')\n",
      "\n",
      "    display_times = [(start_datetime + datetime.timedelta(minutes = 30 + i*60)).strftime('%Y-%m-%d %H:%M') for i in range(0, 24)]\n",
      "    \n",
      "    filename = date + '.json'\n",
      "    data = {\n",
      "        'sites': site_info,\n",
      "        'start_time': start_time,\n",
      "        'display_times': display_times\n",
      "      }\n",
      "    open(filename, 'w').write(json.dumps(data))\n",
      "    print 'Wrote json to %s' % filename\n",
      "\n",
      "write_json_for_date('2014-06-20')\n",
      "#write_json_for_date('2015-02-01')\n",
      "#write_json_for_date('2015-02-02')\n",
      "#write_json_for_date('2015-02-03')\n",
      "#write_json_for_date('2015-02-04')\n",
      "#write_json_for_date('2015-02-05')\n",
      "#write_json_for_date('2015-02-06')\n",
      "#write_json_for_date('2015-02-07')\n",
      "#write_json_for_date('2015-02-08')\n",
      "#write_json_for_date('2015-02-09')\n",
      "#write_json_for_date('2015-02-10')\n",
      "#write_json_for_date('2015-02-11') ##\n",
      "#write_json_for_date('2015-02-12')\n",
      "#write_json_for_date('2015-02-13')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Merging Lawrenceville 2 ACHD into Lawrenceville ACHD\n",
        "  Skipping key name already in dest\n",
        "  Skipping key longitude already in dest\n",
        "  Skipping key id already in dest\n",
        "  Skipping key latitude already in dest\n",
        "Merging Liberty 2 ACHD into Liberty ACHD\n",
        "  Skipping key name already in dest\n",
        "  Skipping key longitude already in dest\n",
        "  Skipping key latitude already in dest\n",
        "  Skipping key id already in dest\n",
        "Wrote json to 2014-06-20.json\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time, end_time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "(1423630800.0, 1423717200.0)"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date = '2015-02-11'\n",
      "start_datetime = dateutil.parser.parse(date).replace(tzinfo=tz.gettz('America/New_York'))\n",
      "[(start_datetime + datetime.timedelta(minutes = 30 + i*60)).strftime('%Y-%m-%d %H:%M') for i in range(0, 24)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "['2015-02-11 00:30',\n",
        " '2015-02-11 01:30',\n",
        " '2015-02-11 02:30',\n",
        " '2015-02-11 03:30',\n",
        " '2015-02-11 04:30',\n",
        " '2015-02-11 05:30',\n",
        " '2015-02-11 06:30',\n",
        " '2015-02-11 07:30',\n",
        " '2015-02-11 08:30',\n",
        " '2015-02-11 09:30',\n",
        " '2015-02-11 10:30',\n",
        " '2015-02-11 11:30',\n",
        " '2015-02-11 12:30',\n",
        " '2015-02-11 13:30',\n",
        " '2015-02-11 14:30',\n",
        " '2015-02-11 15:30',\n",
        " '2015-02-11 16:30',\n",
        " '2015-02-11 17:30',\n",
        " '2015-02-11 18:30',\n",
        " '2015-02-11 19:30',\n",
        " '2015-02-11 20:30',\n",
        " '2015-02-11 21:30',\n",
        " '2015-02-11 22:30',\n",
        " '2015-02-11 23:30']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}